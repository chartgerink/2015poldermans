---
title: "Analyzing DECREASE trials for extent of data fabrication"
author: "Chris HJ Hartgerink, Gerben ter Riet, Marleen Kemper"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  pdf_document:
    keep_tex: yes
    fig_caption: yes
  word_document: default
csl: ../bibliography/ama.csl
bibliography: ../bibliography/library.bib
---

<!-- Introduction
What should be in here?
1. Effectiveness of beta-blockers (decrease probability))
2. Effects of findings of misconduct
3. Why this study. -->
The effect of beta-blockers on perioperative mortality in non-cardiac surgery for patients with coronary artery disease (CAD) has been subject of discussion [@Coleg5210] due to findings of research  misconduct [@commissie2011;@commissie2012;@commissie2013] that were central to claims of their effectiveness [@Devereaux313;@Angeli2010;@bouri2014]. Perioperative mortality is the deathrate of patients during the period of the surgical procedure, which typically includes admission, anaesthesia, surgery, and recovery. This related finding of research misconduct altered meta-analytic results of perioperative mortality in non-cardiac surgery for CAD patients substantively[@bouri2014], where meta-analyses that include these studies conclude that beta-blockers decrease perioperative mortality [@Devereaux313;@Angeli2010;@bouri2014] but a meta-analysis that excludes these studies concludes that beta-blockers increase perioperative mortality[@bouri2014]. 

The trials deemed to be the result of research misconduct were the Dutch DECREASE trials[@commissie2011;@commissie2012;@commissie2013]. The committees that investigated the integrity of the DECREASE studies reported that data fabrication was likely [@commissie2011;@commissie2012;@commissie2013], but that the extent of the data fabrication remained unclear. Given that the inclusion or exclusion of these trials in meta-analyses is crucial in determining whether beta-blockers are indeed effective during the perioperative period and the latest guidelines still recommend the usage of beta-blockers [@esc2014], we aim to estimate the extent of data fabrication in the DECREASE studies [@dunkelgrun2009;@poldermans1999] in order to further this debate. 

The reports on the DECREASE trials primarily focused on the provenance of the raw data and patient files (e.g., informed consent, whether data corresponded to patient files), but neglected the extent to which the DECREASE studies deviated from similar trials that tested the effectiveness of beta-blockers on perioperative mortality. Comparing how (dis)similar trials are from comparable studies is not a new idea [@Buyse1999-jq] and has proven effective in detecting data fabrication [@Knepper2016-la]. This is a different approach from the forensic statistical methods previously applied [@commissie2013], where results within one study are evaluated as opposed to other similar studies. This explains the discrepancy between the feasibility of evaluating the DECREASE trials by the statistical expert of the committee report and ours.

<!-- Hier evt nog wat vertellen over FOI? -->
<!-- Finally, we share our unsuccesful experiences in trying to further investigate the issue by requesting the DECREASE-VI raw data and call for more transparency. -->
To statistically investigate the evidence of data fabrication in the results of the DECREASE studies [@dunkelgrun2009;@poldermans1999], we took three steps. First, we replicate the findings from a 2014 meta-analysis [@bouri2014] that contains sufficient information to estimate the deviation of the DECREASE-I and DECREASE-IV studies from the remaining studies. Second, we evaluate the veracity of the DECREASE-I and DECREASE-IV studies (i.e., the probability that these study findings occurred assuming no data fabrication occurred). Third, we reverse this assumption and assume that data fabrication did occur and estimate how many data points would have to be fabricated to reproduce the results of the DECREASE-I and DECREASE-IV studies if the remaining studies are regarded as estimating the true effect of beta-blockers on perioperative mortality.

# Step 1: reproducing meta-analysis of Bouri et al. (2014)

## Methods

To ensure that we used similar analysis procedures as in the 2014 meta-analysis [@bouri2014], we initially reproduced Bouri et al.'s estimates. This ensured that (1) their results are reproducible and (2) we are using the correct estimates in subsequent steps of our analyses. Using figures 2 and 3 from the original paper [@bouri2014], we extracted the raw event data for the 2 (control vs experimental) by 2 (event or no event) design, which we used to recompute the natural logarithm of the risk ratio and its standard error. The extracted event data is available at [osf.io/aykeh](https://osf.io/aykeh) and our analysis plan was preregistered at [osf.io/vnmzc](https://osf.io/vnmzc).

```{r, echo = FALSE}
suppressPackageStartupMessages(if(!require(metafor)){install.packages('metafor', repos = "http://cran.us.r-project.org")})
suppressPackageStartupMessages(library(metafor))
suppressPackageStartupMessages(if(!require(latex2exp)){install.packages('latex2exp', repos = "http://cran.us.r-project.org")})
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(library(knitr))

x12 <- '../'
# x12 <- ''
bouri_data <- read.csv(sprintf('%sdata/bouri_data.csv', x12))

bouri_data$a <- bouri_data$control_n - bouri_data$control_events
bouri_data$b <- bouri_data$beta_n - bouri_data$beta_events
bouri_data$c <- bouri_data$control_events
bouri_data$d <- bouri_data$beta_event

# Add .5 to studies with zero-counts
bouri_data[c(4, 7, 9), 7:10] <-  bouri_data[c(4, 7, 9), 7:10] + .5

bouri_data$ln_rr <- log((bouri_data$d/(bouri_data$b+bouri_data$d))/(bouri_data$c/(bouri_data$a+bouri_data$c)))
bouri_data$se_ln_rr <- sqrt((1/bouri_data$a)+(1/bouri_data$b)+(1/bouri_data$c)+(1/bouri_data$d))

mod_clean <- metafor::rma(yi = ln_rr,
                          sei = se_ln_rr,
                          method = "REML",
                          data = bouri_data[bouri_data$decrease == 0, ],
                          weighted = TRUE)
mod_dirty <- metafor::rma(yi = ln_rr,
                          sei = se_ln_rr,
                          method = "REML",
                          data = bouri_data[bouri_data$decrease == 1, ],
                          weighted = TRUE)
```

We computed the log risk ratio (i.e., log RR) for each study and pooled these using the `R` package `metafor` [@viechtbauer2010]. We estimated a weighted random-effects model using the restricted maximum-likelihood estimator (i.e., `REML`) [@viechtbauer2005] to estimate the variance of effects. We used the default weighting procedure in the `metafor` package. When there was a zero-count for a cell, .5 was added to each cell of that trial, as is common in meta-analyses on risk- and odds ratios in order to deal with computational artefacts [@agresti2002]. The 2014 meta-analysis [@bouri2014] did not specify the effect variance estimate used; hence, minor discrepancies between our estimates and the original estimates could be due to differences in the estimation procedure.

## Results

We analyzed the RRs to verify the original estimates (Figure 2 of the 2014 meta-analysis [@bouri2014]) and found that we were able to reproduce the estimates for the different sets of studies. Bouri et al. differentiated between the estimates from the non-DECREASE trials and the DECREASE trials. We confirmed the effect size estimates and the variance estimates for both the non-DECREASE- and the DECREASE trials, save for some minor discrepancies due to the estimation method. Table 1 depicts the original and reproduced values for both sets of studies.

```{r echo=FALSE,results='asis'}
table1 <- matrix('', ncol = 5, nrow = 5)
table1[1,3] <- 'Risk ratio'
table1[1,4] <- '$\\tau^2$'
table1[1,5] <- 'Confidence interval'
table1[2,1] <- 'Non-DECREASE'
table1[4,1] <- 'DECREASE'
table1[c(2,4),2] <- 'Original'
table1[c(3,5),2] <- 'Reproduced'
table1[2,3] <- 1.27
table1[2,4] <- 0.00
table1[2,5] <- '[1.01; 1.60]'

table1[3,3] <- round(exp(mod_clean$b), 2)
table1[3,4] <- round(mod_clean$tau2, 2)
table1[3,5] <- sprintf('[%s; %s]', round(exp(mod_clean$ci.lb), 2), round(exp(mod_clean$ci.ub), 2))

table1[4,3] <- 0.42
table1[4,4] <- 0.29
table1[4,5] <- '[0.15; 1.23]'

table1[5,3] <- round(exp(mod_dirty$b), 2)
table1[5,4] <- round(mod_dirty$tau2, 2)
table1[5,5] <- sprintf('[%s; %s]', round(exp(mod_dirty$ci.lb), 2), round(exp(mod_dirty$ci.ub), 2))


kable(table1, caption = 'The original- and reproduced meta-analytic results based on the data provided in the 2014 meta-analysis by Bouri et al.')
```

```{r, echo = FALSE}
mod_mods <- metafor::rma(yi = ln_rr,
                         sei = se_ln_rr,
                         mod =~ as.factor(decrease), # 1 are the decrease studies
                         method = "REML",
                         data = bouri_data,
                         weighted = TRUE)
```

Second, we meta-analyzed all studies combined, including a dummy predictor for the DECREASE and non-DECREASE studies to replicate results presented in Figure 4 of the 2014 meta-analysis [@bouri2014]. Surprisingly, our results showed more evidence for unequal subgroups than the original meta-analysis [@bouri2014]; whereas the original test for subgroup differences was $\chi^2(1)=3.91,p=.05$, our analyses indicated that $\chi^2(1)=`r round(mod_mods$QM, 2)`,p=`r round(mod_mods$QMp, 3)`$. Additionally, the original analyses showed substantial heterogeneity ($I^2=74.4$\%) where we found an $I^2=`r round(mod_mods$I2, 3)`$\%. Different heterogeneity estimators (e.g., `DerSimonian-Laird` instead of `REML`) did not resolve this difference. We tried to clarify these discrepancies by e-mailing the original authors, but failed to receive a response.

<!--- Emailed bouri about this on 2016 03 16 -->

# Step 2: evaluating the veracity of DECREASE studies

Based on the non-DECREASE estimates from Step 1, we estimated the probability of obtaining the results in the DECREASE studies. To this end, we assumed that the non-DECREASE studies provide a valid representation of the true effect of beta-blockers on perioperative mortality (similar to Bouri et al. [@bouri2014]; but see the discussion). The estimated probability is also known as the veracity of the data [@peters2015], which indicates the probability of the observed data under a given true effect. We assumed that the non-DECREASE studies estimate the true population distribution of effects, not perturbed by publication bias due to statistical (non)significance. Publication bias was assumed to not be a problem because a substantial number of nonsignificant effects are included in the dataset (9 of 11 nonsignificant).

## Method

```{r, echo = FALSE}
mod_clean_pred <- predict(mod_clean)
mod_dirty_pred <- predict(mod_dirty)
```

Based on the estimated mean log RR and its credible interval in the non-DECREASE studies, we computed the probability of log RR in the DECREASE trials. The estimates of the non-DECREASE studies were obtained from Step 1, which include the estimated log RR (i.e., `r round(mod_clean$b, 2)`), and its 95\% credibility interval as provided by the package `metafor` (i.e., `r sprintf('[%s; %s]', round(mod_clean_pred$cr.lb, 3), round(mod_clean_pred$cr.ub, 3))`. We assumed a normal distribution of population effects with the estimated effect as the mean of the distribution. The 95\% credibility interval denotes the bounds of the normal distribution that covers 95\% of the density, where the standard deviation is calculated as the distance from the mean to either bound, divided by 1.96. This allows for an approximation of the population effect distribution, as depicted in Figure `r fig <- 1; fig`. 

```{r, echo = FALSE}
temp <- seq(-3, 3, .001)

pdf(sprintf('%sfigures/fig1.pdf', x12), width = 8, height = 6)
par(mar = c(.5,0,0,0))
plot(x = temp,
     dnorm(x = temp,
           mod_clean$b,
           (mod_clean_pred$pred - mod_clean_pred$cr.lb)/1.96),
     type = 'l',
     bty = 'n',
     xaxt = 'n',
     yaxt = 'n',
     xlab = '',
     ylab = '')

points(x = bouri_data$ln_rr[bouri_data$decrease == 1][1], y = 0)
text(x = bouri_data$ln_rr[bouri_data$decrease == 1][1],
     y = .15,
     labels = 'DECREASE-I',
     cex = .8)

points(x = bouri_data$ln_rr[bouri_data$decrease == 1][2], y = 0)
text(x = bouri_data$ln_rr[bouri_data$decrease == 1][2],
     y = -.1,
     labels = 'DECREASE-IV',
     cex = .8)

invisible(dev.off())
```

```{r figure 1, fig.cap = 'Density plot of the estimated true effect distribution based on the non-DECREASE studies only, with the position of the DECREASE studies highlighted.', out.width = '80%', fig.asp = .5, fig.align = 'center', echo = FALSE}
knitr::include_graphics(sprintf("%sfigures/fig1.pdf", x12))
```

Based on the estimated effect distribution from the non-DECREASE trials, we calculated the probability of each DECREASE trial result, or a more extreme result. In other words, we computed the *p*-value for the hypothesis that the DECREASE studies occurred based on the information available from the other trials. 

## Results

```{r, echo = FALSE}
prob <- pnorm(q = bouri_data$ln_rr[bouri_data$decrease == 1],
              mod_clean_pred$pred,
              (mod_clean_pred$pred - mod_clean_pred$cr.lb)/1.96) * 2
```

Figure `r fig` indicates that the DECREASE trials are highly unlikely under the estimated effect distribution based on the non-DECREASE trials. More specifically, DECREASE-I (or more extreme) has a probability of $`r prob[1]`$ (less than 1 in a quintillion) and DECREASE-IV $`r prob[2]`$ (3 in a billion), which indicates they are unlikely to come from the same population effect distribution as the non-DECREASE trials. Observing two of such extremely unlikely results in this population effect distribution jointly is highly improbable, $`r prob[1]*prob[2]`$. This would therefore indicate that the DECREASE trials are severely different from the non-DECREASE studies.

Results from Step 1 indicated that no variance (i.e., homogeneity; $\tau^2=0$) of the effects was observed; given the small number of studies included (i.e., `r mod_clean$k`) this estimate is uncertain, however. We conducted sensitivity analyses in order to see how dependent results are on the specific heterogeneity estimate. The probability of observing both DECREASE trials stays approximately below 1 out of 1000 until the variance estimate is .25, and the probability stays approximately below 1 out of 100 until the variance estimate is .43 (see Figure `r fig <- fig + 1; fig`). Further research would be worthwhile to better estimate the variance in the effects of beta-blockers on perioperative mortality and what moderators affect the effect size (e.g., type of beta-blocker).

```{r, echo = FALSE}
pdf(sprintf('%sfigures/fig2.pdf', x12), width = 8, height = 6)
tau2 <- seq(0, 2, .01)

prob_sens_t <- NULL
i <- 1

for (t2 in tau2){
  mod_clean_sens <- metafor::rma(yi = ln_rr,
                                 sei = se_ln_rr, tau2 = t2,
                                 method = "REML",
                                 data = bouri_data[bouri_data$decrease == 0, ],
                                 weighted = TRUE)
  mod_clean_pred_sens <- predict(mod_clean_sens)
  prob_sens <- pnorm(q = bouri_data$ln_rr[bouri_data$decrease == 1],
                     mod_clean_pred_sens$pred,
                     (mod_clean_pred_sens$pred - mod_clean_pred_sens$cr.lb)/1.96) * 2
  prob_sens_t[i] <- prob_sens[1] * prob_sens[2]
  
  i <- i + 1
}

par(mar = c(4, 4, 0, 0))
plot(tau2, prob_sens_t, type = 'l', xlab = TeX('$\\tau^2$'),
     ylab = 'p-value', bty = 'n')
points(x = tau2[c(26, 44)], y = prob_sens_t[c(26, 44)])
text(x = tau2[c(26, 44)],
     y = prob_sens_t[c(26, 44)] + .02,
     labels = c(TeX('$\\tau^2=.25$'), TeX('$\\tau^2=.43$')),
     cex = .8)
invisible(dev.off())
```
```{r figure 2, fig.cap = 'Sensitivity analyses for the p-value that indicates the probability of observing the results from the DECREASE studies, or more extreme results, based on the estimated true effect (non-DECREASE trials) and the accompanying variance estimate.', out.width = '80%', fig.asp = .5, fig.align = 'center', echo = FALSE}
knitr::include_graphics(sprintf("%sfigures/fig2.pdf", x12))
```

# Step 3: estimating the amount of fabricated data

We estimated the number of data points that would need to be fabricated to arrive at the estimates from the DECREASE studies, given that the non-DECREASE studies represent the true effect. This assumes, in contrast to Step 2, that the DECREASE studies in fact contain fabricated data. The estimates from Step 3 provide an indication of the extent of the potential data fabrication in the DECREASE studies under this assumption [@commissie2011;@commissie2012;@commissie2013;@bouri2014].

## Method

In order to estimate the number of fabricated data points, we estimated the population effect distribution of mortality (in log odds) per condition in both the DECREASE- and non-DECREASE studies separately. For each of these, we ran a meta-analysis applying the same, verified methods used in Step 1. These separate analyses were necessary, considering that Step 1 only provided estimates of the overall risk ratio but not the odds conditional on exposure (beta-blocker or control) and study set (DECREASE- versus non-DECREASE). This resulted in four meta-analytic mortality estimates with an accompanying variance. Throughout the simulations, we use the point estimates to simulate genuine- and fabricated data, but supplement this by using the distribution estimates as sensitivity analyses.

We applied the inversion method to estimate the number of fabricated data points in the DECREASE-I and DECREASE-IV studies [@casella2002]. With the inversion method, we iteratively hypothesized that *X* out of *N* data points were fabricated (i.e., $0, 1, ..., N$) in the DECREASE-I and DECREASE-IV studies separately. For each study, we simulated 10,000 datasets per hypothesis of *X* fabricated data points and *N-X* genuine data points. For each simulated dataset (exact simulation procedure in next paragraph), we determined the likelihood of the results with 
\begin{equation}
\label{eq1}
L(\theta|\pi_{E},\pi_{C})=\pi_{E}^{n_{11}}(1-\pi_{E})^{n_{12}} \times \pi_{C}^{n_{21}}(1 - \pi_{C})^{n_{22}}
\end{equation}
where $\pi_{E}$ indicates the mortality rate of the experimental condition as drawn from the meta-analytic effect distribution ($\pi_{C}$ indicates the same for the control condition). The likelihood was computed under both the fabricated effect estimates (i.e., $L_{fabricated}$) and the genuine data (i.e., $L_{genuine}$). Table 2 indicates which cell sizes the various $n_{XX}$ refer to within the (simulated) data. We compared the likelihoods to determine whether the simulated data were more likely to arise from the genuine- ($L_{genuine}$) or from the fabricated data ($L_{fabricated}$). Note that using the likelihoods for this is a minor deviation from the preregistration, where we initially planned on using $p$-value comparisons ([osf.io/vnmzc](https://osf.io/vnmzc)).

```{r echo=FALSE,results='asis'}
table2 <- matrix('', ncol = 3, nrow = 3)
table2[1,1] <- ''
table2[1,2] <- 'Dead'
table2[1,3] <- 'Alive'

table2[2,1] <- 'Beta-blockers'
table2[2,2] <- '$n_{11}$'
table2[2,3] <- '$n_{12}$'

table2[3,1] <- 'Control'
table2[3,2] <- '$n_{21}$'
table2[3,3] <- '$n_{22}$'

kable(table2, caption = 'Outcome possibilities within a simulated 2 (beta-blocker v control) by 2 (dead v alive) clinical trial.')
```

For each hypothesis of *X* out of *N* fabricated datapoints, we computed the probability that the fabricated data are more likely than the genuine data ($p_F=P(L_{fabricated}>L_{genuine})$).  Based on $p_F$, we computed the confidence interval for $X$ (i.e., $X_{LB};X_{UB}$). For a 95\% confidence interval, the lowerbound is equal to the $p_F$ closest to .025, whereas the upperbound is equal to the $p_F$ closest to .975.

We computed $p_F$ for all possibilities of *X* out of *N* fabricated datapoints in 10,000 randomly generated datsets, which were generated in three steps. For each dataset we: 

1. Sampled (without replacement) *X* fictitious participants that would be the result of data fabrication (sampled across all conditions). 

2. Determined the population mortality rate for each condition (i.e., for each cell as in Table 3). The meta-analytic point estimate was used or a population effect was randomly drawn from the meta-analytic effect distribution.

3. Simulated the number of deaths for the different conditions using a binomial distribution based on the mortality rate as determined in 2, resulting in the cell counts as in Table 3.

Based on the meta-analytic effect from 2 and the cell sizes from 3, we computed the  likelihoods of $L_{fabricated}$ and $L_{genuine}$ using Equation \ref{eq1}. As mentioned before, $p_F$ was computed as the probability that the data were more likely under the estimates resulting from the (allegedly) fabricated data (i.e., the DECREASE trials) than under the estimates resulting from the genuine data (i.e., the non-DECREASE trials; $p_F=P(L_{fabricated}>L_{genuine})$).

## Results

```{r, echo = FALSE}
# Log odds for control conditions
numerator <- bouri_data$c/(bouri_data$a + bouri_data$c)
denominator <- 1 - (bouri_data$c / (bouri_data$a + bouri_data$c))
bouri_data$ln_odds_cont <- log(numerator / denominator)

bouri_data$se_ln_odds_cont <- 1 / ((bouri_data$a + bouri_data$c) * 
                                     (bouri_data$c / (bouri_data$a + bouri_data$c)) *
                                     (1 - (bouri_data$c / (bouri_data$a + bouri_data$c))))

# Log odds for beta-blocker conditions
numerator <- bouri_data$d/(bouri_data$b + bouri_data$d)
denominator <- 1 - (bouri_data$d / (bouri_data$b + bouri_data$d))
bouri_data$ln_odds_beta <- log(numerator / denominator)

bouri_data$se_ln_odds_beta <- 1 / ((bouri_data$b + bouri_data$d) * 
                                     (bouri_data$d / (bouri_data$b + bouri_data$d)) *
                                     (1 - (bouri_data$d / (bouri_data$b + bouri_data$d))))

mod_cont <- rma(yi = ln_odds_cont,
                sei = se_ln_odds_cont,
                data = bouri_data[bouri_data$decrease == 0, ],
                method = 'REML',
                weighted = TRUE)
mod_beta <- rma(yi = ln_odds_beta,
                sei = se_ln_odds_beta,
                data = bouri_data[bouri_data$decrease == 0, ],
                method = 'REML',
                weighted = TRUE)

mod_cont_decrease <- rma(yi = ln_odds_cont,
                         sei = se_ln_odds_cont,
                         data = bouri_data[bouri_data$decrease == 1, ],
                         method = 'REML',
                         weighted = TRUE)
mod_beta_decrease <- rma(yi = ln_odds_beta,
                         sei = se_ln_odds_beta,
                         data = bouri_data[bouri_data$decrease == 1, ],
                         method = 'REML',
                         weighted = TRUE)

cont_pred <- predict(mod_cont)$pred
cont_cr_lb <- predict(mod_cont)$cr.lb
cont_cr_ub <- predict(mod_cont)$cr.ub
cont_var <- (cont_pred - cont_cr_lb) / 1.96

beta_pred <- predict(mod_beta)$pred
beta_cr_lb <- predict(mod_beta)$cr.lb
beta_cr_ub <- predict(mod_beta)$cr.ub
beta_var <- (beta_pred - beta_cr_lb) / 1.96

cont_decrease_pred <- predict(mod_cont_decrease)$pred
cont_decrease_cr_lb <- predict(mod_cont_decrease)$cr.lb
cont_decrease_cr_ub <- predict(mod_cont_decrease)$cr.ub
cont_decrease_var <- (cont_decrease_pred - cont_decrease_cr_lb) / 1.96

beta_decrease_pred <- predict(mod_beta_decrease)$pred
beta_decrease_cr_lb <- predict(mod_beta_decrease)$cr.lb
beta_decrease_cr_ub <- predict(mod_beta_decrease)$cr.ub
beta_decrease_var <- (beta_decrease_pred - beta_decrease_cr_lb) / 1.96

iterations <- 10000

# Uncomment if you want to rerun the inversion method
# > t1
# [1] "2016-03-29 13:34:31 CEST"
# > t2
# [1] "2016-03-29 15:55:27 CEST"
# t1 <- Sys.time()
# source('../functions/inversion_method_uncertain.R')
# # source('functions/inversion_method_uncertain.R')
# t2 <- Sys.time()
# source('../functions/inversion_method_certain.R')
# # source('functions/inversion_method_certain.R')
# t3 <- Sys.time()

load('../data/result_almost_i_uncertain')
load('../data/result_almost_i_certain')

load('../data/result_almost_iv_uncertain')
load('../data/result_almost_iv_certain')

# load('data/result_almost_i_uncertain')
# load('data/result_almost_i_certain')
# 
# load('data/result_almost_iv_uncertain')
# load('data/result_almost_iv_certain')

res_i_uncertain <- result_almost_i_uncertain / iterations
res_i_certain <- result_almost_i_certain / iterations
  
res_iv_uncertain <- result_almost_iv_uncertain / iterations
res_iv_certain <- result_almost_iv_certain / iterations

res_i_uncertain_lb <- NULL
res_i_uncertain_ub <- NULL
res_i_certain_lb <- NULL
res_i_certain_ub <- NULL

res_iv_uncertain_lb <- NULL
res_iv_uncertain_ub <- NULL
res_iv_certain_lb <- NULL
res_iv_certain_ub <- NULL

j <- 1
for (i in seq(0, 1, .01))
{
  lb <- .5 - (i / 2)
  ub <- .5 + (i / 2)
  
  res_i_uncertain_lb[j] <- which(abs(res_i_uncertain - lb) == min(abs(res_i_uncertain - lb)))[1]
  res_i_uncertain_ub[j] <- which(abs(res_i_uncertain - ub) == min(abs(res_i_uncertain - ub)))[1]
  
  
  res_i_certain_lb[j] <- which(abs(res_i_certain - lb) == min(abs(res_i_certain - lb)))[1]
  res_i_certain_ub[j] <- which(abs(res_i_certain - ub) == min(abs(res_i_certain - ub)))[1]
  
  
  res_iv_uncertain_lb[j] <- which(abs(res_iv_uncertain - lb) == min(abs(res_iv_uncertain - lb)))[1]
  res_iv_uncertain_ub[j] <- which(abs(res_iv_uncertain - ub) == min(abs(res_iv_uncertain - ub)))[1]
  
  res_iv_certain_lb[j] <- which(abs(res_iv_certain - lb) == min(abs(res_iv_certain - lb)))[1]
  res_iv_certain_ub[j] <- which(abs(res_iv_certain - ub) == min(abs(res_iv_certain - ub)))[1]
  
  
  j <- j + 1
}

# Manually correct some errors in the bound seeking from the loop above
# errors arise from Monte Carlo error:
# .022 .021 .023 <-- selects .021, but should select 0
# considering that running >10000 takes ages and does not guarantee
# it will fix the problem, I do it like this
res_i_uncertain_lb <- ifelse(res_i_uncertain_lb == 4, 0, res_i_uncertain_lb)
res_i_certain_lb <- ifelse(res_i_certain_lb == 1, 0, res_i_certain_lb)
res_iv_uncertain_lb <- ifelse(res_iv_uncertain_lb == 8, 0, res_iv_uncertain_lb)
res_iv_certain_lb[98:101] <- 0
res_iv_certain_ub[98:101] <- 1067

# Old plotting. Saved temporarily. Ignore if forgotten to delete.
# pdf(sprintf('%sfigures/fig3.pdf', x12), width = 8, height = 6)
# 
# par(mar = c(2.5, 5, 0.5, 0.5),
#     mfrow = c(2, 2))
# plot(res_i_uncertain,
#      xlab = '',
#      ylab = 'Proportion',
#      ylim = 0:1,
#      type = 'l',
#      lty = 1)
# lines(res_i_certain, pch = 2, type = 'l', lty = 2)
# abline(h = c(.025, .975), lty = 3)
# 
# legend(x = 0, y = .9, bty = 'n', lty = c(1, 2), legend = c("Distribution estimate", "Point estimate"))
# 
# plot(x = NA, y = NA,
#      xlim = c(0,1), ylim = c(0, length(res_i_certain)),
#      xlab = "Confidence in %", ylab = "Data points fabricated")
# 
# lines(x = seq(0, 1, .01), y = res_i_uncertain_lb)
# lines(x = seq(0, 1, .01), y = res_i_uncertain_ub)
# lines(x = seq(0, 1, .01), y = res_i_certain_lb, lty = 2, pch = 2, type = 'l')
# lines(x = seq(0, 1, .01), y = res_i_certain_ub, lty = 2, pch = 2, type = 'l')
# abline(v = .95, lty = 3)
# 
# # clip(x1 = 0, x2 = length(res_i_uncertain), y1 = 0.025, y2 = .975)
# # abline(v = which(abs(res_i_uncertain - .975) == min(abs(res_i_uncertain - .975))), lty = 1)
# # abline(v = which(abs(res_i_certain - .975) == min(abs(res_i_certain - .975))), lty = 2)
# 
# par(mar = c(4, 5, 0.5, 0.5))
# plot(res_iv_uncertain,
#      xlab = 'Data-points fabricated',
#      ylab = 'Proportion',
#      ylim = 0:1,
#      type = 'l',
#      lty = 1)
# lines(res_iv_certain, pch = 2, type = 'l',
#       lty = 2)
# abline(h = c(.025, .975), lty = 3)
# 
# plot(x = NA, y = NA,
#      xlim = c(0,1), ylim = c(0, length(res_iv_certain)),
#      xlab = "Confidence in %", ylab = "Data points fabricated")
# 
# lines(x = seq(0, 1, .01), y = res_iv_uncertain_lb)
# lines(x = seq(0, 1, .01), y = res_iv_uncertain_ub)
# lines(x = seq(0, 1, .01), y = res_iv_certain_lb, lty = 2, pch = 2, type = 'l')
# lines(x = seq(0, 1, .01), y = res_iv_certain_ub, lty = 2, pch = 2, type = 'l')
# abline(v = .95, lty = 3)
# invisible(dev.off())

pdf(sprintf('%sfigures/fig3.pdf', x12), width = 8, height = 6)

par(mar = c(5, 5, 2.5, 0),
    mfrow = c(2, 2))
plot(res_i_uncertain,
     xlab = 'Data points fabricated',
     ylab = 'Proportion',
     main = 'DECREASE-I',
     ylim = 0:1,
     type = 'l',
     lty = 1)
lines(res_i_certain, pch = 2, type = 'l', lty = 2)
abline(h = c(.025, .975), lty = 3)

legend(x = 0, y = .9, bty = 'n', lty = c(1, 2), legend = c("Distribution estimate", "Point estimate"))

par(mar = c(5, 0, 2.5, 0.5))

plot(res_iv_uncertain,
     xlab = 'Data-points fabricated',
     ylab = '',
     main = 'DECREASE-IV',
     ylim = 0:1,
     yaxt = 'n',
     type = 'l',
     lty = 1)
lines(res_iv_certain, pch = 2, type = 'l',
      lty = 2)
abline(h = c(.025, .975), lty = 3)

par(mar = c(4, 5, 0.5, 0))

plot(x = NA, y = NA,
     xlim = c(0,1), ylim = c(0, length(res_i_certain)),
     xlab = "Confidence in %", ylab = "Data points fabricated")

lines(x = seq(0, 1, .01), y = res_i_uncertain_lb)
lines(x = seq(0, 1, .01), y = res_i_uncertain_ub)
lines(x = seq(0, 1, .01), y = res_i_certain_lb, lty = 2, pch = 2, type = 'l')
lines(x = seq(0, 1, .01), y = res_i_certain_ub, lty = 2, pch = 2, type = 'l')
abline(v = .95, lty = 3)

# clip(x1 = 0, x2 = length(res_i_uncertain), y1 = 0.025, y2 = .975)
# abline(v = which(abs(res_i_uncertain - .975) == min(abs(res_i_uncertain - .975))), lty = 1)
# abline(v = which(abs(res_i_certain - .975) == min(abs(res_i_certain - .975))), lty = 2)

par(mar = c(4, 0, 0.5, 0.5))

plot(x = NA, y = NA,
     xlim = c(0,1), ylim = c(0, length(res_iv_certain)),
     xlab = "Confidence in %", ylab = "",
     yaxt = 'n')

lines(x = seq(0, 1, .01), y = res_iv_uncertain_lb)
lines(x = seq(0, 1, .01), y = res_iv_uncertain_ub)
lines(x = seq(0, 1, .01), y = res_iv_certain_lb, lty = 2, pch = 2, type = 'l')
lines(x = seq(0, 1, .01), y = res_iv_certain_ub, lty = 2, pch = 2, type = 'l')
abline(v = .95, lty = 3)
invisible(dev.off())
```

```{r figure 3, fig.cap = 'Inversion method results used to estimate the number of data points fabricated in the DECREASE-I and DECREASE-IV trials. The top row panels indicate $p_F$ (y-axis) for all $X$ out of $N$ fabricated data points (x-axis). The bottom row indicates the estimated number of fabricated data points (y-axis) when varying the degree of confidence (x-axis). Dotted lines indicate the bounds for a 95 percent CI.', out.width = '80%', fig.asp = .5, fig.align = 'center', echo = FALSE}
knitr::include_graphics(sprintf("%sfigures/fig3.pdf", x12))
```

For DECREASE-I, the 95\% confidence interval for the estimated number of fabricated data points is [`r which.min(res_i_certain - .025) - 1` - `r which.max(res_i_certain - .975) - 1`] or [`r which.min(res_i_uncertain - .025) - 1` - `r which.max(res_i_uncertain - .975) - 1`] when based on a point estimate or a more uncertain distribution estimate, respectively. The left column of Figure 3 depicts the $p_F$ per *X* fabricated data points (top panel) and the bounds of the confidence interval when the degree of confidence is altered (lower panel). Staying clearly between the dotted lines in the top panel, depicting the 95\% CI (top: .975; bottom: .025), it becomes apparent that the degree of uncertainty is too high to make any estimates with a high degree of confidence. This due to the small sample size (i.e., $N=`r length(res_i_certain) - 1`$) combined with the availability of just the summary results. Only when the degree of confidence is lowered to around 75\% does the interval not span the entire sample size. As such, based on the summary results, little can be said about the extent of the data fabrication that occurred in the DECREASE-I trial, affirming the conclusions of the original committee report [@commissie2013]. 

For DECREASE-IV, the 95\% confidence interval for the estimated number of fabricated data points is [`r which.min(res_iv_certain - .025) - 1` - `r which.max(res_iv_certain - .975) - 1`] or [`r which.min(res_iv_uncertain - .025) - 1` - `r which.max(res_iv_uncertain - .975) - 1`] when based on a point estimate or a more uncertain distribution estimate, respectively. The minor difference between the estimates indicates that there is a high degree of confidence that data fabrication did occur. Nonetheless, the range of potentially fabricated data points is still estimated at approximately 1000; this indicates that the summary results are insufficient to provide more than an estimated lowerbound. However, this does indicate that it is possible not all data were fabricated (i.e., $N=`r length(res_iv_certain) - 1`$), increasing the importance of data provenance to discern between genuine and falsified data. This affirms the conclusion of the scientific integrity committee that the results of the DECREASE-IV trial are "scientifically incorrect" (p.11) [@commissie2012]. 

The results also highlight that, despite the lack of availability of raw data, summary results from larger samples result in more certainty about the estimated number of fabricated data points using the inversion method. This is due to decreased standard errors at the population level of the effects, providing less variation in the simulations and resulting in higher sensitivity. However, this also highlights that in order to prevent detection it would be in the fabricators' interest to fabricate small studies (assuming the fabricator wants to remain undetected). 

# Discussion

<!---
GtR
1. Is natuurlijk wle gevaarlijk als we in gedachten nemen dat DECREASE op een heel
andere benadering van perioperatieve blockade is gestoeld.
Ik begrijp dat een dergeliijk bezwaar het fundament van onze exercitie aantast,
maar we moeten daar goed over nadenken.
In discussie kunnen we zeggen:
Decrease approach is niet wezenlijk anders dan non-decrease enonze analyse geldt
Decrease appraoch is wezenijk anders qua aanpak en effect en daarom kunnen
verschillen te wijten zijin aan fraude maar ook aan beter effect en die twee
kunnen wij niet scheiden. 
Nieuwe decrease aanpak trials zijj nodig (zie ook paper uit NTvG van Marleen)
2. In Discussion we must debate the statistical aspects and the content aspects where DECREASE have a very different approach to the prevention of periop mortality than non-DECREASE.
This makes our own work much more difficult to interpret
-->

Throughout steps 1, 2, and 3 we provided evidence that the , the DECREASE trials are highly unlikely to come from a genuine data pool, and that there is a high degree of confidence that data was fab

<!--Move to discussion?-->
We note that the variance in the non-DECREASE studies was estimated at 0, indicating that there would supposedly be a fixed-effect of beta-blockers on perioperative mortality despite differences in type of beta-blockers administered and dosage/duration of the beta-blockers. This homogeneity is not excessive, considering that the *Q*-statistic is sufficiently large to begin with (i.e., `r round(mod_clean$QE, .2)`).[@ioannidis2006] Nonetheless, the original non-DECREASE trials show substantive variation in the treatment, for example the usage of different beta-blockers (i.e., atenolol, propanolol, bisoprolol, metoprolol) in different (standardized) dosages, which have been argued to affect effectiveness of the interventions[@klei2015]. As such, the lack of heterogeneity can be genuine and indicative of no difference between these different beta-blockers and their dosage/duration. Nonetheless, it could also be due to uncertainty in the estimate of heterogeneity $\tau^2$ due to the small set of studies included.

However, due to various types of beta-blockers, evidence might be moderated [@klei2015].

Additionally, during the process of this research paper, the first author tried to ascertain a raw dataset of one of the DECREASE trials via a Freedom of Information request (FOI). The Erasmus MC refused to share these data

The ESC/ESA guidelines from 2014[@] were published in October 2014, whereas the meta-analysis indicating a reversal of the effectiveness of beta-blockers was published online at the end of July 2013[@]. The committee revising the ESC/ESA guidelines stated: "The respective writing committees independently performed their literature review and analysis, and then developed their recommendations."[@Kristensen_2014] Regardless, the revisions made "recommend continuation of beta-blocker therapy in the perioperative period in patients currently receiving this medication" and do not recommend the "initiation of beta-blockers in patients undergoing low-risk surgery"[@guarracino2015]. Review pieces note no substantial clinical change with respect to beta-blockers.[@Port2016;@Kristensen2016]

The revisions to the ESC/ESA guidelines do not discourage use of beta-blockers, despite the obvious lack of evidence for systematic evidence of the effectiveness of beta-blockers to reduce perioperative mortality in trials. If anything, the trials show that insufficient evidence has been collected. This becomes apparent when the estimated effect for all non-DECREASE trials is inspected; its *p*-value is .04, which is insufficient evidence and is actually more likely when there is no effect in of beta-blockers in reality[@van_Aert_2016]

In order , the first author tried to ascertain the data of DECREASE VI, the only DECREASE study for which a dataset is still available[@Coleg5210] via a Freedom of Information (FOI) request. Within this paper, we inspected data anomalies based on summary results, which contain less information to be able to determine what isThe board of Erasmus Medical Centrum (EMC) was unwilling to comply with this request, stating that it was not a matter of the board. Regardless of whether it was a board related matter, 

# References